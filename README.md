Run  `traintestsplit.py` once to generate train test split then do not run again

TODO: train test split nyt. 

## File directory
By default things are run using the thesis virtual environment. Newssentiment requires a lower version of python so thus has a unique venv. 

- `scripts`: python files to be run once to usually to generate new data or upload data.
    - `data2s3.py`: Uploads most of the contents of local data folder to s3 with the same key path. Run for the newyorktime directory right now. 
    - `dategen.py`: makes of `data/{publication}/date/date.csv` folder w/ date standardization. Run on both the main hong kong dataset and the alibaba subset for nyt, gt, and cd.
    - `hkmask.py`: makes `data/{publication}/hk_mask/hkmask.csv`. Searches for "hong kong" in headline or body. Most important for SCMP and HKFP which didn't have hong kong filter in original data collection process, but run on everything to double check hong kong relevance. 
    - `logconfig.py`: logging configuration plain and simple
    - `ner_run.py`: soact NER for ppl, places, things in body of text. 
    -  `nytmergeclean.py` creates nyt_full? currently missing code I think? Was run once on hk nyt stuff 6/18/22
    - `polifilter.py`: see polifilter main directory instead
    - `quote_extract.ipynb`: uses gender gap tracker package to extract quotes. Was originally run over sagemaker. 
    - `scmpquotenerconosolidation.py`: scmp quote and ner was originally done on yearly data; this script runs across the years and turns into single csv. 
    - `traintestsplit.py`: .1:.9 split for train test.
- `R`
    - `contextrpoc.R`: plays with contextual embeddings following [guide](https://cran.r-project.org/web/packages/conText/vignettes/quickstart.pdf). Embedding regression not yet implemented. Runs fairly fast on just SCMP data.
    - `fitmodel.RDS` original full run on training data 6/24/22; has 50 topics and converged at 50 iterations. Generated by...
    - `stminit.R`: generates model from training data and runs experiment on test set. Some visualizations too. 
    - `textProcessor.R`: fork of stm function but allows for post stemming stop word removal.
    - `printsagelabels.R`: extention of stm function but allows for filtering of topics. 
    - `mystops.txt`: one line per stop word; random stop words. Fairly conservative removal
    - `polistops.txt`: failyr aggressive stop word removal for non-political type words like sports words. 
- `thesisutils`: pacakge with publications helper fns, s3 fns, and general timing functions. Run `pip install .` from this directory to install latest version. 
- `eda`: many of the graphs in report come from this folder. 
    - `geenraleda.py`: renames cols and merges data a la stm R code. 
    - `maskeda.py`: Explores the effect of subsetting the data on political articles and dropping columns based on keywords
    - `quoteanalysis.py`: Does some merging to try to get most commonly quoted entities. 6/25/22 only tested on 21 scmp data. Takes >3 m to run on that subset :(
    - `section_eda.py`: placeholder for `thesis/polinews/poli-section_poc` where we look at common sections and intersection with poli filter. 
- `pocs`: Proof of concepts which get turned into scripts/edas to be run. These
    should generally not be run because they will overwrite script data which
    is main source of truth but are good to play around. 
    - `poc\ner_poc`: use spacy en large to get ppl, places, & things.
    - `poc\newsentiment_poc`: use [NewsMTSC](https://github.com/fhamborg/NewsMTSC/tree/main/NewsSentiment) to get prob a target is +, - , or = (newssentiment environ)
    - `poc\polifilter_poc`: Placeholder for the `thesis/polinews` poc because we need to run from the [Political-News-Filter](https://github.com/lukasgebhard/Political-News-Filter) repo and can't install package. Gets probability article is about politics. 
    - `poc\quote_poc`: quote extraction inspired by [GenderGapTracker](https://github.com/sfu-discourse-lab/GenderGapTracker). 
- `tmp_quote21.csv`: a random file from scmp `quote _analaysis.py` which has single word quote speakers consoldiated into longer ones when possible. 