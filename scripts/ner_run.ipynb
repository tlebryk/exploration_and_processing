{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e306c43-e0b6-40a4-b1e1-c1159a21b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from thesisutils import utils\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging, logging.config\n",
    "from pathlib import Path\n",
    "import logconfig\n",
    "\n",
    "lgconf = logconfig.logconfig(\"ner_run\")\n",
    "logging.config.dictConfig(lgconf.config_dct)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "tqdm.pandas()\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762dc467-271c-44e4-a64f-2b06eee5da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# df = pd.read_csv(r\"C:\\Users\\tlebr\\OneDrive - pku.edu.cn\\Thesis\\data\\scmp\\2021.csv\")\n",
    "# %%\n",
    "ner_filter = [\n",
    "    \"DATE\",\n",
    "    \"WORK_OF_ART\",\n",
    "    \"PERCENT\",\n",
    "    \"QUANTITY\",\n",
    "    \"TIME\",\n",
    "    \"MONEY\",\n",
    "    # \"LAW\",\n",
    "    \"LANGUAGE\"\n",
    "    \"ORDINAL\",\n",
    "    \"CARDINAL\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfb255e-c455-48aa-b643-8f06b0c14284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Searches for people, orgs, nations, places, laws using spacy NER.\"\"\"\n",
    "def ner(row, text_col, uid_col, publication):\n",
    "    \"\"\"Get all nes from a a row in the main df where row has a text_col\"\"\"\n",
    "    dct_ls = []\n",
    "    doc = nlp(row[text_col], disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "    ents = doc.ents\n",
    "    for ent in ents:\n",
    "        if ent.label_ not in ner_filter:\n",
    "            dct = {\n",
    "\n",
    "                \"entity\" : ent.text,\n",
    "                \"label_\" : ent.label_,\n",
    "                # \"label\" : ent.label,\n",
    "                \"start\" : ent.start,\n",
    "                \"end\" : ent.end,\n",
    "                publication.uidcol: row[uid_col],\n",
    "                \"publication\": publication.name,\n",
    "                # \"year\": year,\n",
    "            }\n",
    "            dct_ls.append(dct)\n",
    "    return dct_ls\n",
    "\n",
    "# HKMAIN RUN #####################################\n",
    " # %%\n",
    "def run(input_df, output_name, text_col, **kwargs):\n",
    "    \"\"\"takes df and creates ner df out of it.\"\"\"\n",
    "    input_df[text_col] = input_df[text_col].astype(str)\n",
    "    ner_dcts = input_df.progress_apply(\n",
    "        lambda row: ner(row, text_col, **kwargs), axis=1\n",
    "    )\n",
    "    nerdf = pd.json_normalize(ner_dcts.explode()).dropna().convert_dtypes()\n",
    "    nerdf.to_csv(output_name)\n",
    "    return nerdf\n",
    "\n",
    "# %%\n",
    "def ner_run(publication, key=None, bucket=\"newyorktime\"):\n",
    "    \"\"\"Wraps run by getting df and other vars for a spec publication.\"\"\"\n",
    "    # publication = utils.publications[\"nyt\"]\n",
    "    logger.info(\"working on %s\", publication.name)\n",
    "    df = utils.get_df(publication)\n",
    "    kwargs = {\n",
    "        # \"year\": year,\n",
    "        \"publication\": publication,\n",
    "        \"text_col\": publication.textcol,\n",
    "        \"uid_col\": publication.uidcol\n",
    "    }\n",
    "    if not key:\n",
    "        key = f\"{publication.name}/ner/ner_full.csv\"\n",
    "    output_name = os.path.join(utils.ROOTPATH, key)\n",
    "    if not os.path.exists(output_name):\n",
    "        os.makedirs(os.path.dirname(output_name))\n",
    "    nerdf = utils.timeit(run, df, output_name, **kwargs)\n",
    "    utils.df_to_s3(nerdf, key, bucket=bucket)\n",
    "# # %%\n",
    "# ner_run(utils.publications['chinadaily'])\n",
    "# # %%\n",
    "# ner_run(utils.publications['nyt'])\n",
    "# # %%\n",
    "# # 3 run starting here\n",
    "# ner_run(utils.publications['hkfp'])\n",
    "# # %%\n",
    "# ner_run(utils.publications['globaltimes'])\n",
    "# # %%\n",
    "# ner_run(utils.publications['scmp'])\n",
    "\n",
    "# %%\n",
    "    # print(f\"{list(ent.sents)=}\")\n",
    "\n",
    "    # print(f\"{ent.ent_id=}\")\n",
    "    # print(f\"{ent.ent_id_=}\")\n",
    "    # print(f\"{ent.id=}\")\n",
    "    # print(f\"{ent.kb_id_=}\")\n",
    "    # print(f\"{ent.kb_id=}\")\n",
    "    # print(f\"{ent.sentiment=}\")\n",
    "    # print(f\"{ent.start=}\")\n",
    "    # print(f\"{ent.end=}\")\n",
    "    # print(f\"{ent.vector=}\")\n",
    "    # print(f\"{ent.text_with_ws=}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a82dc1ba-56a6-4810-864f-90fc088151ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIBABA RUN #####################################\n",
    "# %%\n",
    "def ner_run_baba(pub, local=True, outputkey=None):\n",
    "    \"\"\"Copy of ner_run but with new functionality to \n",
    "        fit baba data.\n",
    "    \"\"\"\n",
    "    # publication = utils.publications[\"nyt\"]\n",
    "    logger.info(\"working on %s\", pub.name)\n",
    "    if local:\n",
    "        df = utils.get_df(pub)\n",
    "    else:\n",
    "        df = utils.read_df_s3(object_key=f\"{pub.name}/{pub.name}_full.csv\", bucket=\"aliba\")\n",
    "    kwargs = {\n",
    "        # \"year\": year,\n",
    "        \"publication\": pub,\n",
    "        \"text_col\": pub.textcol,\n",
    "        \"uid_col\": pub.uidcol\n",
    "    }\n",
    "    # s3 key\n",
    "    outputkey = f\"{pub.name}/ner/ner_full.csv\"\n",
    "    output_name = outputkey\n",
    "    # output_name = os.path.join(utils.ROOTPATH, \"baba\", outputkey)\n",
    "    if not os.path.exists(output_name):\n",
    "        os.makedirs(os.path.dirname(output_name))\n",
    "    nerdf = utils.timeit(run, df, output_name, **kwargs)\n",
    "    utils.df_to_s3(nerdf, outputkey, bucket=\"aliba\")\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5534eace-7d79-48b4-b111-592ab26f818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 19:01:18,961 [INFO] __main__: working on hkfp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:07<00:00, 22.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run took 7.750878297999975 secs\n"
     ]
    }
   ],
   "source": [
    "pub = utils.publications['hkfp']\n",
    "ner_run_baba(pub, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c40240-9496-4b0c-b3f4-fb02c16f9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 19:08:05,263 [INFO] __main__: working on globaltimes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4688/4688 [02:54<00:00, 26.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run took 175.97956715200007 secs\n"
     ]
    }
   ],
   "source": [
    "pub = utils.publications['globaltimes']\n",
    "ner_run_baba(pub, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7473224a-acd9-48ee-8577-67720813dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 19:11:03,345 [INFO] __main__: working on chinadaily\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7547/7547 [04:26<00:00, 28.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run took 269.0859223130001 secs\n"
     ]
    }
   ],
   "source": [
    "pub = utils.publications['chinadaily']\n",
    "ner_run_baba(pub, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94736383-cbda-4932-8505-af4a731af310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 19:15:35,706 [INFO] __main__: working on scmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5007/5007 [03:27<00:00, 24.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run took 209.62251676699998 secs\n"
     ]
    }
   ],
   "source": [
    "pub = utils.publications['scmp']\n",
    "ner_run_baba(pub, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d5178-f6d4-4231-b8f1-45536aa330d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_nlp",
   "language": "python",
   "name": "conda_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
